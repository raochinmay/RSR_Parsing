{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract Address\n",
    "- Step 1: Using regex to extract address info in the resume\n",
    "- Step 2: Parsing the address components to a dictionary for look up, using usaddress package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import usaddress\n",
    "\n",
    "#Extract the address from resume file \n",
    "def extract_address (text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    regex = re.compile(r\"[0-9]+ .*[.,-]? .*[.,-]? ([A-Z]{2}|\\w+)[.,-]? [0-9]{5}(-[0-9]{4})?\")\n",
    "    result = re.search(regex, text)\n",
    "    if result:\n",
    "        result = result.group()\n",
    "    return result\n",
    "\n",
    "#Parse the components\n",
    "def parse_address(result):\n",
    "    address = usaddress.tag(result)\n",
    "    return address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========Testing========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "text = []\n",
    "text.append('''Hi, Mr. Sam D. Richards lives here, 44 West 22nd Street, New York, NY 12345-4567. \n",
    "Can you contact him now? If you need any help, call me on 123 456 7891''')\n",
    "text.append(''' ABEBAW AYELE\n",
    "6040 14th St NW Washington DC 20011\n",
    "202-629-7212 \tabex72@gmail.com''')\n",
    "text.append('''Amanda Yu\n",
    "    \t                       9700 Skyhill Way· Rockville· MD 20850·301-502-8705·yubo0107@hotmail.com''')\n",
    "text.append('''Miguel Lorenzo M. Aviles\n",
    "1644 New Windsor Ct\n",
    "Crofton, Maryland 21114\n",
    "(703) 501-1932\n",
    "maviles@umd.edu\n",
    "''')\n",
    "text.append('''Alexander Berger\n",
    "3711 Campus Drive\n",
    "College Park, MD 20742\n",
    "240-338-2206\n",
    "alexfberger@gmail.com\n",
    "Objective\n",
    "I am seeking an entry-level position where I can use my design and software skills to provide better and more intuitive application design for customers and for my team.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 West 22nd Street, New York, NY 12345-4567\n",
      "6040 14th St NW Washington DC 20011\n",
      "9700 Skyhill Way· Rockville· MD 20850\n",
      "1644 New Windsor Ct Crofton, Maryland 21114\n",
      "3711 Campus Drive College Park, MD 20742\n"
     ]
    }
   ],
   "source": [
    "#Extracting Address\n",
    "address=[]\n",
    "for i in range(len(text)):\n",
    "    address.append(extract_address(text[i])) \n",
    "    print(address[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: (OrderedDict([('AddressNumber', '44'), ('StreetNamePreDirectional', 'West'), ('StreetName', '22nd'), ('StreetNamePostType', 'Street'), ('PlaceName', 'New York'), ('StateName', 'NY'), ('ZipCode', '12345-4567')]), 'Street Address')\n",
      "Person 2: (OrderedDict([('AddressNumber', '6040'), ('StreetName', '14th'), ('StreetNamePostType', 'St'), ('StreetNamePostDirectional', 'NW'), ('PlaceName', 'Washington'), ('StateName', 'DC'), ('ZipCode', '20011')]), 'Street Address')\n",
      "Person 3: (OrderedDict([('AddressNumber', '9700'), ('StreetName', 'Skyhill'), ('StreetNamePostType', 'Way·'), ('PlaceName', 'Rockville·'), ('StateName', 'MD'), ('ZipCode', '20850')]), 'Street Address')\n",
      "Person 4: (OrderedDict([('AddressNumber', '1644'), ('StreetName', 'New Windsor'), ('StreetNamePostType', 'Ct'), ('PlaceName', 'Crofton'), ('StateName', 'Maryland'), ('ZipCode', '21114')]), 'Street Address')\n",
      "Person 5: (OrderedDict([('AddressNumber', '3711'), ('StreetName', 'Campus'), ('StreetNamePostType', 'Drive'), ('PlaceName', 'College Park'), ('StateName', 'MD'), ('ZipCode', '20742')]), 'Street Address')\n"
     ]
    }
   ],
   "source": [
    "#Parse the component\n",
    "address_components = []\n",
    "for i in range(len(text)):\n",
    "    address_components.append(parse_address(address[i]))\n",
    "    print(\"Person {0}: {1}\".format(i+1, parse_address(address[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Person 1\n",
      "Address: 44 West 22nd Street \n",
      "City: New York\n",
      "State: NY\n",
      "ZipCode: 12345-4567\n",
      "\n",
      "Person 2\n",
      "Address: 6040 14th St NW \n",
      "City: Washington\n",
      "State: DC\n",
      "ZipCode: 20011\n",
      "\n",
      "Person 3\n",
      "Address: 9700 Skyhill Way· \n",
      "City: Rockville·\n",
      "State: MD\n",
      "ZipCode: 20850\n",
      "\n",
      "Person 4\n",
      "Address: 1644 New Windsor Ct \n",
      "City: Crofton\n",
      "State: Maryland\n",
      "ZipCode: 21114\n",
      "\n",
      "Person 5\n",
      "Address: 3711 Campus Drive \n",
      "City: College Park\n",
      "State: MD\n",
      "ZipCode: 20742\n"
     ]
    }
   ],
   "source": [
    "#Print the result\n",
    "for i in range(len(address_components)):\n",
    "    person = list(address_components[i][0].items())\n",
    "    Address=''\n",
    "    for j, item in enumerate(person):\n",
    "        if person[j][0] == 'PlaceName':\n",
    "            Placename = person[j][1]\n",
    "        elif person[j][0] == 'StateName':\n",
    "            State = person[j][1]\n",
    "        elif person[j][0] == 'ZipCode':\n",
    "            ZipCode = person[j][1]\n",
    "        else:\n",
    "            Address += person[j][1] + ' '\n",
    "\n",
    "    print('\\nPerson {}'.format(i+1))\n",
    "    print(\"Address: {}\".format(Address))\n",
    "    print(\"City: {}\".format(Placename))\n",
    "    print(\"State: {}\".format(State))\n",
    "    print(\"ZipCode: {}\".format(ZipCode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract Skills \n",
    "\n",
    "Combined both supervised and unsupervised learning to train the parsing algorithm. \n",
    "- Unsupervised: Word2Vec, KMeans (built in the above sections)\n",
    "- Supervised: NaiveBayes \n",
    "\n",
    "In order to implement supervised learning, we need to prepare a big training data of skills to label the resume document for training purposes. First, we create a big list of all possible skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "data = pd.read_excel(\"Skills.xlsx\", header=0)\n",
    "skill_list = list(data['Skill Names'])\n",
    "skill_list = set(skill_list)\n",
    "skill_list= [skill.lower() for skill in skill_list]\n",
    "sorted(skill_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 2: we import the resume document and preprocessing the text by removing all stopwords, special characters and lower all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "filename ='all_text1.txt'\n",
    "trained_resume_path = os.path.join('Trained Resumes', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#resume_text = docx2txt.process(test_resume_path)\n",
    "resume_text = open(trained_resume_path, 'r', encoding='utf_8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "special_characters = ['!','#', '$', '%','&','*','-', '/', '=','?',\n",
    "                      '^','.','_','`', '{', '|', '}','~', \"'\", ',', '(',')', ':', '•', '§' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Processing text \n",
    "\n",
    "def resume_processing (resume_text):\n",
    "    #tokenize sentences\n",
    "    resume_sents = nltk.sent_tokenize(resume_text)\n",
    "\n",
    "    #tokenize words\n",
    "    resume_words = [nltk.word_tokenize(sent) for sent in resume_sents]\n",
    "    \n",
    "    #remove stopwords and special characters\n",
    "    processed_resume=[]\n",
    "    for sentence in resume_words:\n",
    "        sent = [w.lower() for w in sentence \n",
    "                          if w.lower() not in stopwords.words('english') and w.lower() not in special_characters]\n",
    "        processed_resume.append(sent)\n",
    "    \n",
    "    return processed_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: We apply bigram and trigram model for all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_resume = resume_processing(resume_text)\n",
    "unigram_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "#Create bigram model\n",
    "bigram_model_path = 'bigram_model'\n",
    "\n",
    "bigram_model = Phrases(unigram_resume)\n",
    "bigram_model.save(bigram_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create bigram words\n",
    "def create_bigram (unigram_resume):\n",
    "    bigram_model = Phrases.load(bigram_model_path)\n",
    "    bigram_resume = [bigram_model[sentence] for sentence in unigram_resume]\n",
    "    return bigram_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_resume = create_bigram(unigram_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create trigram model \n",
    "trigram_model_path = 'trigram_model'\n",
    "\n",
    "trigram_model = Phrases(bigram_resume)\n",
    "trigram_model.save(trigram_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create trigram words\n",
    "def create_trigram (bigram_resume):\n",
    "    trigram_model = Phrases.load(trigram_model_path)\n",
    "    trigram_resume = [trigram_model[sentence] for sentence in bigram_resume]\n",
    "    return trigram_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_resume = create_trigram(bigram_resume)\n",
    "trigram_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparing the string, we need to normalize the text by removing the \"_\" in bigram and trigram words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Normalize bigram/trigram words \n",
    "def normalize_words (trigram_resume):\n",
    "    for sentence in trigram_resume:\n",
    "        for i, word in enumerate(sentence):   \n",
    "            if len(re.findall(r'\\w+\\_\\w+', word))!= 0:\n",
    "                sentence[i] = re.sub('_', ' ', word)\n",
    "    return trigram_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_resume = normalize_words(trigram_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For supervised learning, we need to have a trained data with the words being labeled. In step 4: we labeled all the words with 'skill' and 'not skill' in resume text by comparing string words with skill list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label skills in the resume\n",
    "def labeled_word (sentence):\n",
    "    labels=[]\n",
    "    for word in sentence:\n",
    "        if word in skill_list:\n",
    "            labels.append((word, 'skill'))\n",
    "        else:\n",
    "            labels.append((word, 'not skill'))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_words=[labeled_word(sentence) for sentence in normalized_resume]\n",
    "labeled_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Extract features to fit in the algorithm\n",
    "- First feature is to check if the word is in skill list\n",
    "- Second feature examines the probability that the top 25 similar words generated from Word2Vec model will be in skill list\n",
    "- Third feature is to check of the word is in skill cluster generated from KMeans\n",
    "- Fourth and fifth feature are to check if the previous word and next word would be in skill list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def similar_prob(word):\n",
    "    count = 0\n",
    "    terms = get_related_terms(word,25)\n",
    "    for w in terms:\n",
    "        if skill_series.isin([w]).any():\n",
    "            count+=1\n",
    "    return count/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_skill_cluster(word):\n",
    "    if word in skills:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract featurres of skills \n",
    "def extract_features (sentence, i):\n",
    "    features={}\n",
    "    #first feature: evaluate if that word is in skill list\n",
    "    features[\"({})in_skill_list\".format(sentence[i])]= (sentence[i] in skill_list)\n",
    "    \n",
    "    if sentence[i] in res2vec.wv.vocab:\n",
    "        features[\"probality_of_similar_words_skills\"] = similar_prob(sentence[i])\n",
    "        features[\"in_skill_cluster\"] = in_skill_cluster(sentence[i])\n",
    "    \n",
    "    #if the word is in begining of the sentence, return <Start> for prev_word\n",
    "    if i==0 and len(sentence)-1 != 0:\n",
    "        features[\"prev_word_in_skill_list\"]= '<Start>'\n",
    "        features[\"next_word_in_skill_list\"]= (sentence[i+1] in skill_list)\n",
    "    \n",
    "    #if the word is in begining of the sentence, return <End> for next_word\n",
    "    elif i == len(sentence)-1 and  i != 0:\n",
    "        features[\"prev_word_in_skill_list\"]= (sentence[i-1] in skill_list)\n",
    "        features[\"next_word_in_skill_list\"]= '<End>'\n",
    "    \n",
    "    #if the sentence has only 1 word, return False for both prev_word and next_word\n",
    "    elif i==0 and len(sentence)-1 == 0:\n",
    "        features[\"prev_word_in_skill_list\"]= False\n",
    "        features[\"next_word_in_skill_list\"]= False\n",
    "    else:\n",
    "        features[\"prev_word_in_skill_list\"]= (sentence[i-1] in skill_list)\n",
    "        features[\"next_word_in_skill_list\"]= (sentence[i+1] in skill_list)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are then stored in feature set and split into training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "featuresets=[]\n",
    "for labeled_sent in labeled_words:\n",
    "    unlabeled_sent = [word[0] for word in labeled_sent]\n",
    "    for i, (w, label) in enumerate(labeled_sent):\n",
    "        featuresets.append((extract_features(unlabeled_sent, i), label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the features in a file\n",
    "featuresets_file = 'features_file.txt'\n",
    "file = open(featuresets_file, 'w', encoding='utf_8')\n",
    "file.write('\\n'.join('%s %s' % item for item in featuresets ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = int(len(featuresets)*0.1)\n",
    "train_set = featuresets[size:]\n",
    "test_set = featuresets[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training data to train the NaiveBayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy of the model based on labeled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show most informative features to determine if a word is skill or not. As seen from the result, a word that has more similiar words as skill will have a good chance to be a skill in the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========Testing========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file =['sampleMechanical Engineering Resume.txt', 'desktop support engineer resume.txt','Henrydao -Resume.txt',\n",
    "            'Electrical Engineering Student Resume.txt','Technical Consultant Resume.txt','Technical Manager Resume.txt',\n",
    "           'Technical Support Resume.txt', 'Technical Writer Resume.txt', 'Yiyang (Eric) Zhou Resume 2017 Spring.txt']\n",
    "\n",
    "def extract_skills(normalized_test_res, resume_number, filename):\n",
    "    skills =[]\n",
    "    for sent in normalized_test_res:\n",
    "        for (i,_) in enumerate(sent):\n",
    "            if classifier.classify(extract_features(sent, i))=='skill':\n",
    "                skills.append(sent[i])\n",
    "                extracted_skills = set(skills)\n",
    "    print('\\nResume {}:{} ({} skills)\\n'.format(resume_number+1,filename, len(extracted_skills)), extracted_skills)\n",
    "    \n",
    "for i, filename in enumerate(test_file):\n",
    "    test_resume_path= os.path.join('Test Resumes', filename)\n",
    "\n",
    "    test_resume = open(test_resume_path, 'r').read()\n",
    "    unigram_test_res = resume_processing(test_resume)\n",
    "    bigram_test_res = create_bigram(unigram_test_res)\n",
    "    trigram_test_res = create_trigram(bigram_test_res)\n",
    "    normalized_test_res = normalize_words(trigram_test_res)\n",
    "    extract_skills(normalized_test_res, i, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Companies\n",
    "We will mostly use the text segmentation to strip out the block of text that contains work experience information based on the section headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "\n",
    "filename = 'BrandonThomasResume.txt'\n",
    "#Open file\n",
    "def open_file(filename):\n",
    "    resume = open(filename, 'r', errors='ignore').read()\n",
    "    return resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resume = open_file(filename)\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of all possible header for work experience to identify the upper bound of the text block and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Import different put of experience headers\n",
    "data = pd.read_excel(\"Work Experience.xlsx\", header=0)\n",
    "experience_list = list(data['Example'])\n",
    "experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy.process import dedupe\n",
    "\n",
    "#Find the experience header\n",
    "def find_exp_header (resume):\n",
    "    exp_header_list=[]\n",
    "    for word in experience_list:\n",
    "        if resume.find(word) != -1:\n",
    "            exp_header_list.append(word)\n",
    "    \n",
    "    #remove duplicates of experience header\n",
    "    exp_header = list(dedupe(exp_header_list))\n",
    "    return exp_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_header = find_exp_header(resume)\n",
    "exp_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_header = (exp_header[0], resume.find(exp_header[0]))\n",
    "exp_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of all section headers to identify the next section header being the lower bound of the text block and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "#List of all sections in a typical resume\n",
    "section_list =['EDUCATION', 'Education', 'Skills', 'SKILLS', 'VOLUNTEER EXPERIENCE', 'Volunteer Experience',\n",
    "              'Technical Skills', 'TECHNICAL SKILS', 'SUMMARY', 'summary', 'Professional Summary', 'PROFESSIONAL SUMMARY',\n",
    "              'DEMONSTRATED SKILLS', 'Demonstrated Skills', 'Additional Information', 'ADDITIONAL INFORMATION', \n",
    "               'Leadership Experience', 'LEADERSHIP EXPERIENCE', 'REFERENCES', 'References', \n",
    "               'Certificates & Trainings', 'CERTIFICATE & TRAININGS', 'TRAINING', 'Training', 'Certificate', 'CERTIFICATE', \n",
    "               'RELEVANT COURSES', 'LANGUAGES', 'Relevant Courses', 'Languages', 'LEADERSHIP AND VOLUNTEER EXPERIENCE',\n",
    "               'Leadership and Volunteer Experience', 'LEADERSHIP & VOLUNTEER EXPERIENCE', 'Leadership & Volunteer Experience',\n",
    "               'EDUCATION AND TRAINING', 'Education and Training', 'Key Projects', 'KEY PROJECTS', 'RELEVANT ACADEMIC PROJECTS', \n",
    "               'Relevant Academic Projects', 'ACADEMIC PROJECTS', 'Academic Projects', 'EXTRACURRICULAR ACTIVITIES', \n",
    "               'Extracurricular Activities'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find next section header\n",
    "def find_next_section (resume):\n",
    "    #Find all capitalized words\n",
    "    next_section_upper = re.findall(r'([A-Z]{3,}( [A-Z]+)?( [A-Z]+)?( [A-Z]+)?)', \n",
    "                                   resume[(exp_header[1] + len(exp_header[0])+ 1):])\n",
    "    next_section_upper = list((itertools.chain.from_iterable(next_section_upper)))\n",
    "    \n",
    "    #Find all words with the first letter capitalized\n",
    "    next_section_lower = re.findall(r'([A-Z]{1}\\w+( [A-Z]{1}\\w+)?( [A-Z]{1}\\w+)?( [A-Z]{1}\\w+)?)',\n",
    "                                    resume[(exp_header[1] + len(exp_header[0])+ 1):])\n",
    "    next_section_lower = list((itertools.chain.from_iterable(next_section_lower)))\n",
    "    \n",
    "    #Combine into a list\n",
    "    next_section_list = next_section_upper + next_section_lower\n",
    "    \n",
    "    #if one of the items matches items in section list, that item is the next section header\n",
    "    next_section=()\n",
    "    for item in next_section_list:\n",
    "        if item in section_list and (resume[resume.find(item)+len(item)]=='\\n' or resume[resume.find(item)-1]=='\\n'):\n",
    "            next_section = (item, resume.find(item))\n",
    "            break\n",
    "    return next_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_section = find_next_section(resume)\n",
    "next_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_workexp_section(resume):\n",
    "    if next_section:\n",
    "        workexp_section = str(resume[(exp_header[1]+ len(exp_header[0])+ 1):next_section[1]])\n",
    "    else:\n",
    "        workexp_section = str(resume[(exp_header[1]+ len(exp_header[0])+ 1):])\n",
    "    return workexp_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strip out that block of text as work experience info and remove all the details that starts with the bullet points. What information left are company information that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workexp_section = get_workexp_section(resume)\n",
    "workexp_section = workexp_section.split('\\n')\n",
    "workexp_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the detail and get the experience information\n",
    "def get_exp_info(work_exp):\n",
    "    company_info=[]\n",
    "    temp_str=''\n",
    "    for i, sent in enumerate(work_exp):\n",
    "        if sent != '':\n",
    "            #Everything before the bullet will be put into one sentence, for one company\n",
    "            if not sent.startswith(('•','', u'\\uf095', '§', '§')): \n",
    "                temp_str += sent + ' '\n",
    "            else:\n",
    "                if not work_exp[i-1].startswith(('•','', u'\\uf095', '§', '§')):\n",
    "                    company_info.append(temp_str)\n",
    "                    temp_str=''\n",
    "    return company_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_info = get_exp_info(workexp_section)\n",
    "for i, company in enumerate(company_info):\n",
    "    company = company.replace('\\t', '')\n",
    "    print('\\nCompany {}:'.format(i+1), company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step involves using Name-entity recognition to parse the components of company info, including company name, location, duration and the role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def extract_exp_info(company_info, filename):\n",
    "    count = 0\n",
    "    print(filename)\n",
    "    for i, sent in enumerate(company_info):\n",
    "        sent = sent.replace('\\t', '')\n",
    "        parsed_sent = nlp(sent)\n",
    "        print('\\nCompany {}'.format(i+1))\n",
    "        \n",
    "        company=''\n",
    "        location=''\n",
    "        time=''\n",
    "        role=''\n",
    "        for i ,token in enumerate(parsed_sent):\n",
    "            if token.ent_type_ =='ORG':\n",
    "                company += ' ' + str(token)\n",
    "            elif token.ent_type_ =='GPE':\n",
    "                location += ' ' + str(token)\n",
    "            elif token.ent_type_ =='DATE' or token.ent_type_ =='TIME':\n",
    "                time += ' ' + str(token)\n",
    "            elif token.ent_type_ =='':\n",
    "                if str(token).isalpha() and str(token) not in stopwords.words('english'):\n",
    "                    role += ' ' + str(token)\n",
    "        \n",
    "        print('Company: {}'.format(company))\n",
    "        print('Location: {}'.format(location))\n",
    "        print('Time: {}'.format(time))\n",
    "        print('Role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_exp_info(company_info, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parseVirt",
   "language": "python",
   "name": "parsevirt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
