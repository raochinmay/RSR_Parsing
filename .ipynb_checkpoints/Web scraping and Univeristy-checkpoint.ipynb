{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyangzhou/anaconda/lib/python3.5/site-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#all imports\n",
    "\n",
    "#import library for regular expression\n",
    "import re\n",
    "\n",
    "#import pandas analysis tool\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#import string\n",
    "import string\n",
    "\n",
    "#import nltk library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import ngrams\n",
    "\n",
    "import requests\n",
    "\n",
    "#import web scraping tool\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "#the pacakge used to remove duplicates in final step of filtering majors.\n",
    "from fuzzywuzzy.process import dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import path & variables\n",
    "\n",
    "\n",
    "#import and tokenize resume\n",
    "resume_path = r\"/Users/yiyangzhou/Desktop/Yiyang (Eric) Zhou Resume 2017 Fall.txt\"\n",
    "resume_file = open(resume_path).read()\n",
    "resume_file2 = open(resume_path).read()\n",
    "resume_file2 = resume_file2.lower()      #this is a string with the resume file in lower case\n",
    "\n",
    "#import a list of majors using pandas\n",
    "major_df = pandas.read_excel('majors.xlsx')\n",
    "major_df.columns\n",
    "major_file = major_df['Majors'].values\n",
    "major_lower = [item.lower() for item in major_file]     #make everything in the major list in lower case\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')                #this is a tokenizing tool from nltk\n",
    "resume_token = tokenizer.tokenize(resume_file)     #tokenize the original resume string \n",
    "resume_token2 = tokenizer.tokenize(resume_file2)   #tokenized the lower case resume string\n",
    "\n",
    "\n",
    "# create those empty of for majors to store results from later major extraction step. \n",
    "updated_majors1 = []\n",
    "indexes_majors1 = []\n",
    "updated_majors2 = []\n",
    "indexes_majors2 = []\n",
    "updated_majors3 = []\n",
    "indexes_majors3 = []\n",
    "updated_majors4 = []\n",
    "indexes_majors4 = []\n",
    "majors_minors_all = updated_majors1 + updated_majors2 + updated_majors3 + updated_majors4\n",
    "\n",
    "\n",
    "#import a list of universities\n",
    "university_df1 = pandas.read_excel('China_University.xlsx')\n",
    "university_df2 = pandas.read_excel('India_University.xlsx')\n",
    "university_df3 = pandas.read_excel('US_University.xlsx')\n",
    "university_file1 = university_df1['Universities'].values\n",
    "university_file2 = university_df2['Universities'].values\n",
    "university_file3 = university_df3['Universities'].values\n",
    "university_lower1 = [item.lower() for item in university_file1]\n",
    "university_lower2 = [item.lower() for item in university_file2]\n",
    "university_lower3 = [item.lower() for item in university_file3]\n",
    "university_combined = university_lower1 + university_lower2 + university_lower3    # this is the final university list that we refer to \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yiyang(Eric)Zhou\n"
     ]
    }
   ],
   "source": [
    "#extract name \n",
    "\n",
    "# We use the location of name to do the extraction.\n",
    "\n",
    "\n",
    "# This function find the first letter on the first line of resume\n",
    "def extract_first_name(resume):\n",
    "    name = resume.split('\\n', 1)[0]\n",
    "    first_name = name.split(' ', 1)[0]\n",
    "    return (first_name)\n",
    "    print (first_name)\n",
    "\n",
    "#This function find the last letter on the first line of resume\n",
    "def extract_last_name(resume):\n",
    "    name = resume.split('\\n', 1)[0]\n",
    "    last_name = name.split(' ', 1)[-1]\n",
    "    return (last_name)\n",
    "    print (last_name)\n",
    "    \n",
    "    \n",
    "#This funciton combines the first name and last name together to get the full name\n",
    "def extract_name(resume):\n",
    "    name = extract_first_name(resume_file) + extract_last_name(resume_file)\n",
    "    print (name)\n",
    "    \n",
    "extract_name(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yiyang.zhou@rhsmith.umd.edu\n"
     ]
    }
   ],
   "source": [
    "#extract email\n",
    "\n",
    "#extract email based on regular expression. Very straight forward. This works very well, not need to further improvement. \n",
    "def extract_email(resume):\n",
    "    regular_expression = re.compile(r\"(\\w+[.|\\w])*@(\\w+[.])*\\w+\", re.IGNORECASE)\n",
    "    result = re.search(regular_expression, resume)\n",
    "    if result:\n",
    "        result = result.group()\n",
    "    print (result)\n",
    "\n",
    "extract_email(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6125162045'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract phone number\n",
    "\n",
    "#This function uses regular expression to find any 10 digits consecutive numbers. Any punctuation in between are removed. \n",
    "def check_phone_number1(resume):\n",
    "    resume2 = \"\".join(c for c in resume if c not in ('!','.','-','(',')',' ','+',))\n",
    "    result = re.findall(r\"\\d{10}\", resume2)                   \n",
    "    result = ''.join(result)\n",
    "    return (result)\n",
    "\n",
    "#This function uses regular expression to find any 11 digits consecutive numbers. Any punctuation in between are removed.\n",
    "def check_phone_number2(resume):\n",
    "    resume2 = \"\".join(c for c in resume if c not in ('!','.','-','(',')',' ','+',))\n",
    "    result = re.findall(r\"\\d{11}\", resume2)                   \n",
    "    result = ''.join(result)\n",
    "    result = result[1:11]\n",
    "    return (result)\n",
    "\n",
    "\n",
    "#This function first call the function that extract 10 digit number. If the result is valid, it returns the result.\n",
    "#If the first function returns an error, it would call the function to extract the 11 digit number and return the result.\n",
    "def extract_phone_number(resume):\n",
    "    try:\n",
    "        return check_phone_number1(resume) \n",
    "        print (check_phone_number1(resume))\n",
    "    except:\n",
    "        return check_phone_number2(resume)\n",
    "        print (check_phone_number2(resume))\n",
    "\n",
    "extract_phone_number(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0\n",
      "0             Southern Illinois University Edwardsville\n",
      "1                                      Rosemont College\n",
      "2                                     Taylor University\n",
      "3                                  Paul Smith's College\n",
      "4                                    Lourdes University\n",
      "5                                 College of St. Joseph\n",
      "6                       North Carolina Wesleyan College\n",
      "7              California State University, Los Angeles\n",
      "8           Queens College, City University of New York\n",
      "9             Maryland University of Integrative Health\n",
      "10                University of California, Los Angeles\n",
      "11                                 Augustana University\n",
      "12                                     Trine University\n",
      "13                                   University of Iowa\n",
      "14                            Trinity Christian College\n",
      "15                                 University of Dayton\n",
      "16                                    Kalamazoo College\n",
      "17                                      Cornell College\n",
      "18                              Central Baptist College\n",
      "19                      University of California, Davis\n",
      "20                University of Massachusetts Dartmouth\n",
      "21                             Wichita State University\n",
      "22                                   Capital University\n",
      "23                                Buffalo State College\n",
      "24                       University of Central Oklahoma\n",
      "25                                       Thomas College\n",
      "26          Baptist Memorial College of Health Sciences\n",
      "27                                  Bluffton University\n",
      "28                       California College of the Arts\n",
      "29                            Art Academy of Cincinnati\n",
      "...                                                 ...\n",
      "1770                           Ohio Northern University\n",
      "1771                                    Roanoke College\n",
      "1772          New York City College of Technology, CUNY\n",
      "1773                     Texas A&M University-Texarkana\n",
      "1774                    Saint Joseph's College of Maine\n",
      "1775                                  Dickinson College\n",
      "1776                    University of Nevada, Las Vegas\n",
      "1777                             Daniel Webster College\n",
      "1778                  University of Wisconsin-Milwaukee\n",
      "1779                                   Walsh University\n",
      "1780                        Virginia Military Institute\n",
      "1781                                      Unity College\n",
      "1782                            Pensacola State College\n",
      "1783                                New England College\n",
      "1784                                   Tusculum College\n",
      "1785                                 Lees-McRae College\n",
      "1786  Rosalind Franklin University of Medicine and S...\n",
      "1787                                     Lasell College\n",
      "1788                      University of Central Florida\n",
      "1789                         University of West Alabama\n",
      "1790                                   Lycoming College\n",
      "1791                             Dixie State University\n",
      "1792                               Centenary University\n",
      "1793                            Gardner-Webb University\n",
      "1794                        Ouachita Baptist University\n",
      "1795                                La Salle University\n",
      "1796                         American Jewish University\n",
      "1797                 University of Wisconsin-Whitewater\n",
      "1798                           The College at Brockport\n",
      "1799                               University of Oregon\n",
      "\n",
      "[1800 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#scraping tools\n",
    "\n",
    "#first get the webpage that we need to scrape\n",
    "r = requests.get('http://www.4icu.org/us/')\n",
    "\n",
    "#find the kind of content we need to scrape by looking at the webpage's source code\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "#further filter the information we need by looking at the page's source code\n",
    "letters = soup.find_all(class_=\"lead\")\n",
    "\n",
    "#create a dictionary of university\n",
    "university = {}\n",
    "for element in letters:\n",
    "    university[element.get_text()] = {}\n",
    "    \n",
    "#get the keys for the university dictionary. The keys are university names    \n",
    "list_keys = [ k for k in university.keys() ]\n",
    "\n",
    "#organize the university into a data frame\n",
    "us_university_list = pd.DataFrame(list_keys)\n",
    "print (us_university_list)\n",
    "\n",
    "#save to an excel file\n",
    "'''\n",
    "writer = pd.ExcelWriter('US_University.xlsx')\n",
    "df.to_excel(writer, index = False)\n",
    "writer.save()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'university of minnesota', 'university of maryland'}\n"
     ]
    }
   ],
   "source": [
    "#Extract University\n",
    "'''\n",
    "We use the extract matching technique to parse universities. \n",
    "\n",
    "First we define series of functions to get n-grams for the resume token and make a list of them.\n",
    "On later step, we can compare the n-grams resume list to the university list to get the matching result. \n",
    "'''\n",
    "\n",
    "def get_bigrams(input):\n",
    "    n = 2\n",
    "    result = []\n",
    "    bigrams = ngrams(input, n)\n",
    "    for grams in bigrams:\n",
    "        x = \"%s %s\" % grams\n",
    "        result.append(x)\n",
    "    return (result)    \n",
    "    print (result)\n",
    "\n",
    "\n",
    "test1 = get_bigrams(resume_token)\n",
    "#print (test1)\n",
    "\n",
    "def get_threegrams(input):\n",
    "    n = 3\n",
    "    result = []\n",
    "    threegrams = ngrams(input, n)\n",
    "    for grams in threegrams:\n",
    "        x = \"%s %s %s\" % grams\n",
    "        result.append(x)\n",
    "    return (result)    \n",
    "    print (result)\n",
    "\n",
    "test2 = get_threegrams(resume_token)\n",
    "#print (test2)\n",
    "\n",
    "def get_fourgrams(input):\n",
    "    n = 4\n",
    "    result = []\n",
    "    fourgrams = ngrams(input, n)\n",
    "    for grams in fourgrams:\n",
    "        x = \"%s %s %s %s\" % grams\n",
    "        result.append(x)\n",
    "    return (result)    \n",
    "    print (result)\n",
    "\n",
    "test3 = get_fourgrams(resume_token)\n",
    "#print (test3)\n",
    "\n",
    "def get_fivegrams(input):\n",
    "    n = 5\n",
    "    result = []\n",
    "    fivegrams = ngrams(input, n)\n",
    "    for grams in fivegrams:\n",
    "        x = \"%s %s %s %s %s\" % grams\n",
    "        result.append(x)\n",
    "    return (result)    \n",
    "    print (result)\n",
    "\n",
    "test4 = get_fivegrams(resume_token)\n",
    "#print (test4)\n",
    "\n",
    "def get_sixgrams(input):\n",
    "    n = 6\n",
    "    result = []\n",
    "    sixgrams = ngrams(input, n)\n",
    "    for grams in sixgrams:\n",
    "        x = \"%s %s %s %s %s %s\" % grams\n",
    "        result.append(x)\n",
    "    return (result)    \n",
    "    print (result)\n",
    "\n",
    "\n",
    "def get_university(a,b):\n",
    "    resume_university=[]\n",
    "    for x in a:\n",
    "        if x in b:\n",
    "            resume_university.append(x)\n",
    "    return (resume_university)\n",
    "    print (resume_university)\n",
    "\n",
    "#combine all the n-grams together    \n",
    "def extract_university(resume_token_lower,university_combined):\n",
    "    unigram_university = get_university(resume_token_lower, university_combined)  \n",
    "    bigram_university = get_university(get_bigrams(resume_token_lower), university_combined)\n",
    "    threegram_university = get_university(get_threegrams(resume_token_lower), university_combined)\n",
    "    fourgram_university = get_university(get_fourgrams(resume_token_lower), university_combined)\n",
    "    fivegram_university = get_university(get_fivegrams(resume_token_lower), university_combined)\n",
    "    sixgram_university = get_university(get_sixgrams(resume_token_lower), university_combined)\n",
    "    combined_university_extraction = set(bigram_university + threegram_university + fourgram_university + fivegram_university + sixgram_university)\n",
    "    print (combined_university_extraction)\n",
    "\n",
    "extract_university(resume_token2,university_combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPA:3.43'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract GPA:\n",
    "#use regular expression technique to extract gpa. works very well.\n",
    "\n",
    "def extract_GPA(resume):\n",
    "    result = re.search(r'(GPA|gpa): ?\\d.\\d{1,}',resume)\n",
    "    if result:\n",
    "        result = result.group(0)\n",
    "    return (result)\n",
    "    print (result)\n",
    "\n",
    "extract_GPA(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['education', 'business', 'information', 'management', 'accounting', 'finance', 'insurance', 'web', 'development', 'home', 'specialist', 'history', 'analyst', 'marketing', 'english', 'information system', 'accounting and finance']\n"
     ]
    }
   ],
   "source": [
    "#extract majors:\n",
    "\n",
    "'''\n",
    "Same as extracting univerity, we use the extract matching technique to parse majors. \n",
    "We already have the n-grams for the resume file.\n",
    "On later step, we can compare the n-grams resume list to the major list to get the matching result. \n",
    "'''\n",
    "\n",
    "\n",
    "def extract_majors(a,b):\n",
    "    majors=[]\n",
    "    for x in a:\n",
    "        if x in b:\n",
    "            majors.append(x)\n",
    "    return (majors)\n",
    "    print (majors)\n",
    "    \n",
    "unigram_major = extract_majors(resume_token2, major_lower)  \n",
    "bigram_major = extract_majors(get_bigrams(resume_token2), major_lower)\n",
    "threegram_major = extract_majors(get_threegrams(resume_token2), major_lower)\n",
    "combined_majors_list = unigram_major + bigram_major + threegram_major\n",
    "\n",
    "# print out the extracted majors from the resume without redundancy. \n",
    "major_distinct = []\n",
    "for i in combined_majors_list:\n",
    "    if i not in major_distinct:\n",
    "        major_distinct.append(i)\n",
    "        \n",
    "#print (combined_majors_list)\n",
    "'''\n",
    "As you can see in the result, it also contains information that are actually not major in my resume. \n",
    "Thus, we use the location (index) of those words in the resume file to further identify whether it's major or not. \n",
    "'''\n",
    "print (major_distinct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'information system': 220, 'accounting': 395, 'business': 144, 'english': 3176, 'marketing': 2670, 'development': 831, 'analyst': 2232, 'home': 986, 'history': 1158, 'management': 305, 'finance': 410, 'education': 108, 'information': 220, 'accounting and finance': 395, 'specialist': 1015, 'web': 735, 'insurance': 426}\n"
     ]
    }
   ],
   "source": [
    "#get major indexes \n",
    "#we get a dictionary of index for the previous major list that we derive from resume. \n",
    "\n",
    "dict = {'Name': 5}\n",
    "for i, element in enumerate(major_distinct):\n",
    "   x = resume_file2.find(element)\n",
    "   dict[element] = x\n",
    "   \n",
    "\n",
    "del dict['Name']\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bachelor of Science\n",
      "358\n",
      "Minor\n",
      "419\n",
      "Master\n",
      "199\n",
      "University\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "#get indexes for specific terms\n",
    "\n",
    "'''\n",
    "We determine to find the location (index) of the following 4 specific terms: bachelor, master, minor, and university.\n",
    "By comparing the indexes of resume's extracted major list to the 4 specific terms, we can get a much better filtered\n",
    "list for majors.\n",
    "'''\n",
    "\n",
    "#bachelor\n",
    "regular_expression = re.compile(r\"/BA|BS|Bachelor of Science|Bachelor of Arts|BBA |B/A|Bachelor of Business Administration/\", re.IGNORECASE)\n",
    "bach_major_result = re.search(regular_expression, resume_file)\n",
    "if bach_major_result:\n",
    "   bach_major_result = bach_major_result.group()\n",
    "print (bach_major_result)\n",
    "\n",
    "if bach_major_result is not None:\n",
    "    bach_major_index = resume_file.find(bach_major_result)\n",
    "    print(bach_major_index)\n",
    "    \n",
    "#minor\n",
    "regular_expression_two = re.compile(r\"minor|Minor\", re.IGNORECASE)\n",
    "minor_result = re.search(regular_expression_two, resume_file)\n",
    "if minor_result:\n",
    "   minor_result = minor_result.group()\n",
    "print (minor_result)\n",
    "\n",
    "if minor_result is not None:\n",
    "    minor_index = resume_file.find(minor_result)\n",
    "    print(minor_index)\n",
    "\n",
    "#master\n",
    "regular_expression_three = re.compile(r\"Master|master\", re.IGNORECASE)\n",
    "master_major_result = re.search(regular_expression_three, resume_file)\n",
    "if master_major_result:\n",
    "   master_major_result = master_major_result.group()\n",
    "print (master_major_result)\n",
    "\n",
    "if master_major_result is not None:\n",
    "    master_major_index = resume_file.find(master_major_result)\n",
    "    print(master_major_index)\n",
    "    \n",
    "#university\n",
    "regular_expression_four = re.compile(r\"university\", re.IGNORECASE)\n",
    "university_major_result = re.search(regular_expression_four, resume_file)\n",
    "if university_major_result:\n",
    "   university_major_result = university_major_result.group()\n",
    "print (university_major_result)\n",
    "\n",
    "if university_major_result is not None:\n",
    "    university_major_index = resume_file.find(university_major_result)\n",
    "    print(university_major_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accounting', 'finance', 'accounting and finance', 'insurance']\n",
      "[395, 410, 395, 426]\n",
      "['information system', 'information']\n",
      "[220, 220]\n",
      "['information system', 'information']\n",
      "[220, 220]\n",
      "['insurance']\n",
      "[426]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Continue with the previous topic, we try to find which majors words appear in the range (100 characters).\n",
    "After the specific terms. If a major term is in that range, it would be saved into a list.\n",
    "If it's not in the range, then the major term is discarded.\n",
    "'''\n",
    "\n",
    "upper_bound1 = bach_major_index+100\n",
    "for k, v in dict.items():\n",
    "   if (bach_major_index < v < upper_bound1):\n",
    "       updated_majors1.append(k)\n",
    "       indexes_majors1.append(v)\n",
    "print(updated_majors1)\n",
    "print(indexes_majors1)\n",
    "\n",
    "upper_bound2 = master_major_index+100\n",
    "for k, v in dict.items():\n",
    "   if (master_major_index < v < upper_bound2):\n",
    "       updated_majors2.append(k)\n",
    "       indexes_majors2.append(v)\n",
    "print(updated_majors2)\n",
    "print(indexes_majors2)\n",
    "\n",
    "upper_bound3 = university_major_index+100\n",
    "for k, v in dict.items():\n",
    "   if (university_major_index < v < upper_bound3):\n",
    "       updated_majors3.append(k)\n",
    "       indexes_majors3.append(v)\n",
    "print(updated_majors3)\n",
    "print(indexes_majors3)\n",
    "\n",
    "upper_bound4 = minor_index+100\n",
    "for k, v in dict.items():\n",
    "   if (minor_index < v < upper_bound4):\n",
    "       updated_majors4.append(k)\n",
    "       indexes_majors4.append(v)\n",
    "print(updated_majors4)\n",
    "print(indexes_majors4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['information system', 'accounting and finance', 'insurance']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "With respect to the output of previous block, we use the fuzzywuzzy pacakge to get rid of redundancy. \n",
    "For example, if a list contains \"accounting\" and \"accounting and finance\", \n",
    "it would only keep the longer term which contains the shorter term.\n",
    "'''\n",
    "majors_minors_all = updated_majors1 + updated_majors2 + updated_majors3 + updated_majors4\n",
    "majors_minors_final_list = list(dedupe(majors_minors_all))\n",
    "print (majors_minors_final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "parseVirt",
   "language": "python",
   "name": "parsevirt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
